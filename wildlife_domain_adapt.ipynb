{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a48e65",
   "metadata": {},
   "source": [
    "Download Missouri Camera Trap Image\n",
    "\n",
    "`aws s3 cp --recursive --no-sign-request \"s3://us-west-2.opendata.source.coop/agentmorris/lila-wildlife/missouricameratraps/images\" \"./images\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183ce259",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_PATH = \"missouri_camera_traps_set1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5caf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def check_greyscale(img, threshold=1):\n",
    "    arr = np.asarray(img, dtype=np.float32)\n",
    "    diff_rg = np.abs(arr[...,0] - arr[...,1])\n",
    "    diff_rb = np.abs(arr[...,0] - arr[...,2])\n",
    "    diff_gb = np.abs(arr[...,1] - arr[...,2])\n",
    "    mean_diff = (diff_rg.mean() + diff_rb.mean() + diff_gb.mean()) / 3.0\n",
    "    return mean_diff < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31993f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "with open(JSON_PATH, 'r') as f:\n",
    "    ann = json.load(f)\n",
    "train_idx, test_idx = train_test_split(range(len(ann[\"images\"])), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78588ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class CCTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Unpaired dataset loader for Caltech Camera Traps for CycleGAN-style training.\n",
    "    Domain A / Domain B splitting is done by image path or metadata.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 image_dir: str,\n",
    "                 json_path: str,\n",
    "                 num_samples: int = None,\n",
    "                 transform: T.Compose = None,\n",
    "                 image_size: int = 1024,\n",
    "                 mode: str = 'train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          image_dir: root directory containing image files (subfolders or flat).\n",
    "          json_path: full path to COCO-style JSON file with metadata.\n",
    "          transform: torchvision transforms to apply.\n",
    "          image_size: target size (will do Resize + CenterCrop or so).\n",
    "          mode: 'train' or 'val' (if you want different behavior).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        self.mode = mode\n",
    "\n",
    "        # Load the annotation JSON\n",
    "        with open(json_path, 'r') as f:\n",
    "            ann = json.load(f)\n",
    "\n",
    "        # Build image info list\n",
    "        # Each item: {'file_name': ..., 'location': <int or str>, ...}\n",
    "        images_info = ann['images']\n",
    "\n",
    "        # Filter images by location into two domains\n",
    "        self.grey_img = []\n",
    "        self.len_grey = 0\n",
    "        self.col_img = []\n",
    "        self.len_col = 0\n",
    "        self.grey_img_cat = []\n",
    "\n",
    "        if mode == \"train\":\n",
    "            selected_indices = train_idx\n",
    "        else:\n",
    "            selected_indices = test_idx\n",
    "\n",
    "        for idx in selected_indices:\n",
    "            img_info = images_info[idx]\n",
    "            # full path\n",
    "            fn = img_info['file_name']\n",
    "            full_path = os.path.join(image_dir, fn)\n",
    "            if not os.path.isfile(full_path):\n",
    "                # skip missing files\n",
    "                continue\n",
    "            if check_greyscale(Image.open(full_path)):\n",
    "                if num_samples is None or self.len_grey < num_samples:\n",
    "                    self.grey_img_cat.append(1 if \"bbox\" in img_info.keys() else 0)\n",
    "                    self.grey_img.append(full_path)\n",
    "                    self.len_grey += 1\n",
    "            else:\n",
    "                if num_samples is None or self.len_col < num_samples:\n",
    "                    self.col_img.append(full_path)\n",
    "                    self.len_col += 1\n",
    "            if num_samples is None:\n",
    "                continue\n",
    "            elif self.len_col >= num_samples and self.len_grey >= num_samples:\n",
    "                break\n",
    "\n",
    "        self.dataset_length = max(self.len_grey, self.len_col)\n",
    "        print(f\"Greyscale img count: {self.len_grey}, Colour img count: {self.len_col}, using dataset length {self.dataset_length}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For unpaired: sample A and B independently (or aligned by idx mod list length)\n",
    "        pathA = self.grey_img[idx % self.len_grey]\n",
    "        pathB = self.col_img[idx % self.len_col]\n",
    "        catA = self.grey_img_cat[idx % self.len_grey]\n",
    "\n",
    "        img_grey = Image.open(pathA).convert('RGB')\n",
    "        img_col = Image.open(pathB).convert('RGB')\n",
    "\n",
    "        img_grey = self.transform(img_grey)\n",
    "        img_col = self.transform(img_col)\n",
    "\n",
    "        return img_grey, img_col, catA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d1420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greyscale img count: 1024, Colour img count: 1024, using dataset length 1024\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CCTDataset(\n",
    "    image_dir= \"images\",\n",
    "    json_path= JSON_PATH,\n",
    "    num_samples=1024,\n",
    "    image_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2e3bc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greyscale img count: 32, Colour img count: 32, using dataset length 32\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CCTDataset(\n",
    "    image_dir= \"images\",\n",
    "    json_path= JSON_PATH,\n",
    "    num_samples=32,\n",
    "    image_size=512,\n",
    "    mode=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99bfe38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbba44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ U-Net Generator\n",
    "# =========================================================\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
    "        super().__init__()\n",
    "        if down:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.use_dropout = use_dropout\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.down1 = UNetBlock(in_channels, 64, down=True)\n",
    "        self.down2 = UNetBlock(64, 128, down=True)\n",
    "        self.down3 = UNetBlock(128, 256, down=True)\n",
    "        self.down4 = UNetBlock(256, 512, down=True, use_dropout=True)\n",
    "        self.down5 = UNetBlock(512, 512, down=True, use_dropout=True)\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up1 = UNetBlock(512, 512, down=False, use_dropout=True)\n",
    "        self.up2 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
    "        self.up3 = UNetBlock(1024, 256, down=False)\n",
    "        self.up4 = UNetBlock(512, 128, down=False)\n",
    "        self.up5 = UNetBlock(256, 64, down=False)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        bottleneck = self.bottleneck(d5)\n",
    "        u1 = self.up1(bottleneck)\n",
    "        u2 = self.up2(torch.cat([u1, d5], 1))\n",
    "        u3 = self.up3(torch.cat([u2, d4], 1))\n",
    "        u4 = self.up4(torch.cat([u3, d3], 1))\n",
    "        u5 = self.up5(torch.cat([u4, d2], 1))\n",
    "        out = self.final(torch.cat([u5, d1], 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669d1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# 2️⃣ PatchGAN Discriminator\n",
    "# =========================================================\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9b5d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 3️⃣ Setup\n",
    "# ================================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "G_XtoY = UNetGenerator().to(device)\n",
    "G_YtoX = UNetGenerator().to(device)\n",
    "D_X = PatchDiscriminator().to(device)\n",
    "D_Y = PatchDiscriminator().to(device)\n",
    "\n",
    "# Losses\n",
    "adv_criterion = nn.MSELoss()\n",
    "cycle_criterion = nn.L1Loss()\n",
    "\n",
    "# Optimizers\n",
    "opt_G = optim.Adam(\n",
    "    list(G_XtoY.parameters()) + list(G_YtoX.parameters()),\n",
    "    lr=2e-4, betas=(0.5, 0.999)\n",
    ")\n",
    "opt_D = optim.Adam(\n",
    "    list(D_X.parameters()) + list(D_Y.parameters()),\n",
    "    lr=2e-4, betas=(0.5, 0.999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa8fa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 256/256 [09:43<00:00,  2.28s/it, loss_G=3.006, loss_D=0.182]\n",
      "Epoch [2/10]: 100%|██████████| 256/256 [09:44<00:00,  2.28s/it, loss_G=4.304, loss_D=0.107]\n",
      "Epoch [3/10]: 100%|██████████| 256/256 [09:46<00:00,  2.29s/it, loss_G=3.059, loss_D=0.045] \n",
      "Epoch [4/10]: 100%|██████████| 256/256 [09:39<00:00,  2.27s/it, loss_G=4.606, loss_D=0.158]\n",
      "Epoch [5/10]: 100%|██████████| 256/256 [09:39<00:00,  2.26s/it, loss_G=4.363, loss_D=0.232]\n",
      "Epoch [6/10]: 100%|██████████| 256/256 [09:41<00:00,  2.27s/it, loss_G=3.606, loss_D=0.141]\n",
      "Epoch [7/10]: 100%|██████████| 256/256 [09:38<00:00,  2.26s/it, loss_G=4.132, loss_D=0.029]\n",
      "Epoch [8/10]:  89%|████████▉ | 229/256 [08:40<01:01,  2.29s/it, loss_G=2.863, loss_D=0.048]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# =========================================================\n",
    "# 6️⃣ Training Loop\n",
    "# =========================================================\n",
    "n_epochs = 10\n",
    "lambda_cycle = 10.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "    for real_X, real_Y, _ in loop:\n",
    "        real_X, real_Y = real_X.to(device), real_Y.to(device)\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Generators\n",
    "        # -----------------------\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        fake_Y = G_XtoY(real_X)\n",
    "        fake_X = G_YtoX(real_Y)\n",
    "\n",
    "        # Adversarial losses\n",
    "        loss_G_adv_Y = adv_criterion(D_Y(fake_Y), torch.ones_like(D_Y(fake_Y)))\n",
    "        loss_G_adv_X = adv_criterion(D_X(fake_X), torch.ones_like(D_X(fake_X)))\n",
    "        loss_G_adv = (loss_G_adv_X + loss_G_adv_Y)\n",
    "\n",
    "        # Cycle-consistency\n",
    "        recov_X = G_YtoX(fake_Y)\n",
    "        recov_Y = G_XtoY(fake_X)\n",
    "        loss_cycle = cycle_criterion(recov_X, real_X) + cycle_criterion(recov_Y, real_Y)\n",
    "\n",
    "        # Total Generator Loss\n",
    "        loss_G = loss_G_adv + lambda_cycle * loss_cycle\n",
    "        loss_G.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminators\n",
    "        # -----------------------\n",
    "        opt_D.zero_grad()\n",
    "\n",
    "        # D_X\n",
    "        loss_D_X_real = adv_criterion(D_X(real_X), torch.ones_like(D_X(real_X)))\n",
    "        loss_D_X_fake = adv_criterion(D_X(fake_X.detach()), torch.zeros_like(D_X(fake_X)))\n",
    "        loss_D_X = (loss_D_X_real + loss_D_X_fake) * 0.5\n",
    "\n",
    "        # D_Y\n",
    "        loss_D_Y_real = adv_criterion(D_Y(real_Y), torch.ones_like(D_Y(real_Y)))\n",
    "        loss_D_Y_fake = adv_criterion(D_Y(fake_Y.detach()), torch.zeros_like(D_Y(fake_Y)))\n",
    "        loss_D_Y = (loss_D_Y_real + loss_D_Y_fake) * 0.5\n",
    "\n",
    "        # Total Discriminator Loss\n",
    "        loss_D = loss_D_X + loss_D_Y\n",
    "        loss_D.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "        loop.set_postfix({\n",
    "            \"loss_G\": f\"{loss_G.item():.3f}\",\n",
    "            \"loss_D\": f\"{loss_D.item():.3f}\"\n",
    "        })\n",
    "\n",
    "        # ==============================\n",
    "        # Logging\n",
    "        # ==============================\n",
    "\n",
    "    # Save example outputs\n",
    "    save_image(real_X, f\"out/real_X_epoch{epoch}.png\")\n",
    "    save_image(fake_Y, f\"out/fake_Y_epoch{epoch}.png\")\n",
    "    save_image(recov_X, f\"out/rec_X_epoch{epoch}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G_XtoY.state_dict(), \"model/G_XtoY.pth\")\n",
    "torch.save(G_YtoX.state_dict(), \"model/G_YtoX.pth\")\n",
    "torch.save(D_X.state_dict(), \"model/D_X.pth\")\n",
    "torch.save(D_Y.state_dict(), \"model/D_Y.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c2015f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_XtoY.load_state_dict(torch.load(\"model/G_XtoY.pth\"))\n",
    "G_YtoX.load_state_dict(torch.load(\"model/G_YtoX.pth\"))\n",
    "D_X.load_state_dict(torch.load(\"model/D_X.pth\"))\n",
    "D_Y.load_state_dict(torch.load(\"model/D_Y.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0909994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model v5a.0.1 already exists and is valid at C:\\Users\\aidan\\AppData\\Local\\Temp\\megadetector_models\\md_v5a.0.1.pt\n",
      "PyTorch reports 1 available CUDA devices\n",
      "GPU available: True\n",
      "Bypassing imports for model type yolov5\n",
      "Loading PT detector with compatibility mode classic\n",
      "Loaded image size 1280 from model metadata\n",
      "Using model stride: 64\n",
      "PTDetector using device cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Fusing layers... \n",
      "Model summary: 733 layers, 140054656 parameters, 0 gradients, 208.8 GFLOPs\n",
      "Model summary: 733 layers, 140054656 parameters, 0 gradients, 208.8 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from megadetector.detection import run_detector\n",
    "from megadetector.visualization import visualization_utils as vis_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "detector = run_detector.load_detector('MDV5A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4a7c08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_map = {\"1\": \"Animal\", \"2\": \"Person\", \"3\": 'Vehicle'}\n",
    "\n",
    "def draw_result(img, result, path):\n",
    "    fig, ax = plt.subplots()\n",
    "    for det in result:\n",
    "        x1, y1, w, h = np.array(det[\"bbox\"]) * 512\n",
    "        acc = det[\"conf\"]\n",
    "        cat = cat_map[det[\"category\"]]\n",
    "        rect = patches.Rectangle((x1, y1), w, h, linewidth=1, edgecolor='red', facecolor='none')\n",
    "        ax.text(x1+7, y1-10, f\"{cat}: {acc*100:.1f}%\", color=\"black\", fontsize=8, backgroundcolor=\"red\")\n",
    "        ax.add_patch(rect)\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    plt.savefig(path, bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "75949a66",
   "metadata": {},
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[171]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (real_X, real_Y, cat_X) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[32m      3\u001b[39m     save_image(real_X, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33meval/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_real_X.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     fake_Y = G_XtoY(\u001b[43mreal_X\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m     save_image(fake_Y, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33meval/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_fake_Y.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     img_X = vis_utils.load_image(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33meval/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_real_X.png\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"eval\", exist_ok=True)\n",
    "for i, (real_X, real_Y, cat_X) in enumerate(test_loader):\n",
    "    save_image(real_X, f\"eval/{i:02d}_real_X.png\")\n",
    "    fake_Y = G_XtoY(real_X.to(device))\n",
    "    save_image(fake_Y, f\"eval/{i:02d}_fake_Y.png\")\n",
    "    \n",
    "    img_X = vis_utils.load_image(f\"eval/{i:02d}_real_X.png\")\n",
    "    img_Y = vis_utils.load_image(f\"eval/{i:02d}_fake_Y.png\")\n",
    "\n",
    "    result_X = detector.generate_detections_one_image(img_X)\n",
    "    result_Y = detector.generate_detections_one_image(img_Y)\n",
    "    print(cat_X)\n",
    "    print(result_X)\n",
    "    detections_X = [d for d in result_X['detections'] if d['conf'] > 0.1]\n",
    "    print('Found {} detections above threshold'.format(len(detections_X)))\n",
    "    print(detections_X)\n",
    "    draw_result(img_X, detections_X, f\"eval/{i:02d}_real_X.png\")\n",
    "\n",
    "    print(result_Y)\n",
    "    detections_Y = [d for d in result_Y['detections'] if d['conf'] > 0.1]\n",
    "    print('Found {} detections above threshold'.format(len(detections_Y)))\n",
    "    print(detections_Y)\n",
    "    draw_result(img_Y, detections_Y, f\"eval/{i:02d}_fake_Y.png\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildlife-da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
