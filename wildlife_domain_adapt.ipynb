{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a48e65",
   "metadata": {},
   "source": [
    "Download Caltech Camera Trap Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5caf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def check_greyscale(img, threshold=1):\n",
    "    arr = np.asarray(img, dtype=np.float32)\n",
    "    diff_rg = np.abs(arr[...,0] - arr[...,1])\n",
    "    diff_rb = np.abs(arr[...,0] - arr[...,2])\n",
    "    diff_gb = np.abs(arr[...,1] - arr[...,2])\n",
    "    mean_diff = (diff_rg.mean() + diff_rb.mean() + diff_gb.mean()) / 3.0\n",
    "    return mean_diff < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78588ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class CCTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Unpaired dataset loader for Caltech Camera Traps for CycleGAN-style training.\n",
    "    Domain A / Domain B splitting is done by image path or metadata.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 image_dir: str,\n",
    "                 json_path: str,\n",
    "                 num_samples: int = None,\n",
    "                 transform: T.Compose = None,\n",
    "                 image_size: int = 1024,\n",
    "                 mode: str = 'train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          image_dir: root directory containing image files (subfolders or flat).\n",
    "          json_path: full path to COCO-style JSON file with metadata.\n",
    "          transform: torchvision transforms to apply.\n",
    "          image_size: target size (will do Resize + CenterCrop or so).\n",
    "          mode: 'train' or 'val' (if you want different behavior).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        self.mode = mode\n",
    "\n",
    "        # Load the annotation JSON\n",
    "        with open(json_path, 'r') as f:\n",
    "            ann = json.load(f)\n",
    "\n",
    "        # Build image info list\n",
    "        # Each item: {'file_name': ..., 'location': <int or str>, ...}\n",
    "        images_info = ann['images']\n",
    "\n",
    "        # Filter images by location into two domains\n",
    "        self.grey_img = []\n",
    "        self.len_grey = 0\n",
    "        self.col_img = []\n",
    "        self.len_col = 0\n",
    "\n",
    "        for img_info in images_info:\n",
    "            # full path\n",
    "            fn = img_info['file_name']\n",
    "            full_path = os.path.join(image_dir, fn)\n",
    "            if not os.path.isfile(full_path):\n",
    "                # skip missing files\n",
    "                continue\n",
    "            if check_greyscale(Image.open(full_path)):\n",
    "                if num_samples is None or self.len_grey < num_samples:\n",
    "                    self.grey_img.append(full_path)\n",
    "                    self.len_grey += 1\n",
    "            else:\n",
    "                if num_samples is None or self.len_col < num_samples:\n",
    "                    self.col_img.append(full_path)\n",
    "                    self.len_col += 1\n",
    "\n",
    "            if self.len_col >= num_samples and self.len_grey >= num_samples:\n",
    "                break\n",
    "\n",
    "        self.dataset_length = max(self.len_grey, self.len_col)\n",
    "        print(f\"Greyscale img count: {self.len_grey}, Colour img count: {self.len_col}, using dataset length {self.dataset_length}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For unpaired: sample A and B independently (or aligned by idx mod list length)\n",
    "        pathA = self.grey_img[idx % self.len_grey]\n",
    "        pathB = self.col_img[idx % self.len_col]\n",
    "\n",
    "        img_grey = Image.open(pathA).convert('RGB')\n",
    "        img_col = Image.open(pathB).convert('RGB')\n",
    "\n",
    "        img_grey = self.transform(img_grey)\n",
    "        img_col = self.transform(img_col)\n",
    "\n",
    "        return img_grey, img_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greyscale img count: 512, Colour img count: 512, using dataset length 512\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CCTDataset(\n",
    "    image_dir= \"images\",\n",
    "    json_path= \"caltech_images_20210113.json\",\n",
    "    num_samples=512,\n",
    "    image_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99bfe38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbba44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ U-Net Generator\n",
    "# =========================================================\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
    "        super().__init__()\n",
    "        if down:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.use_dropout = use_dropout\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.down1 = UNetBlock(in_channels, 64, down=True)\n",
    "        self.down2 = UNetBlock(64, 128, down=True)\n",
    "        self.down3 = UNetBlock(128, 256, down=True)\n",
    "        self.down4 = UNetBlock(256, 512, down=True, use_dropout=True)\n",
    "        self.down5 = UNetBlock(512, 512, down=True, use_dropout=True)\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up1 = UNetBlock(512, 512, down=False, use_dropout=True)\n",
    "        self.up2 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
    "        self.up3 = UNetBlock(1024, 256, down=False)\n",
    "        self.up4 = UNetBlock(512, 128, down=False)\n",
    "        self.up5 = UNetBlock(256, 64, down=False)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        bottleneck = self.bottleneck(d5)\n",
    "        u1 = self.up1(bottleneck)\n",
    "        u2 = self.up2(torch.cat([u1, d5], 1))\n",
    "        u3 = self.up3(torch.cat([u2, d4], 1))\n",
    "        u4 = self.up4(torch.cat([u3, d3], 1))\n",
    "        u5 = self.up5(torch.cat([u4, d2], 1))\n",
    "        out = self.final(torch.cat([u5, d1], 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669d1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# 2️⃣ PatchGAN Discriminator\n",
    "# =========================================================\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9b5d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 3️⃣ Setup\n",
    "# ================================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "G_XtoY = UNetGenerator().to(device)\n",
    "G_YtoX = UNetGenerator().to(device)\n",
    "D_X = PatchDiscriminator().to(device)\n",
    "D_Y = PatchDiscriminator().to(device)\n",
    "\n",
    "# Losses\n",
    "adv_criterion = nn.MSELoss()\n",
    "cycle_criterion = nn.L1Loss()\n",
    "\n",
    "# Optimizers\n",
    "opt_G = optim.Adam(\n",
    "    list(G_XtoY.parameters()) + list(G_YtoX.parameters()),\n",
    "    lr=2e-4, betas=(0.5, 0.999)\n",
    ")\n",
    "opt_D = optim.Adam(\n",
    "    list(D_X.parameters()) + list(D_Y.parameters()),\n",
    "    lr=2e-4, betas=(0.5, 0.999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa8fa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 128/128 [04:43<00:00,  2.21s/it, loss_G=5.094, loss_D=0.060]\n",
      "Epoch [2/10]: 100%|██████████| 128/128 [04:48<00:00,  2.26s/it, loss_G=4.369, loss_D=0.510]\n",
      "Epoch [3/10]: 100%|██████████| 128/128 [04:48<00:00,  2.25s/it, loss_G=4.922, loss_D=0.187]\n",
      "Epoch [4/10]: 100%|██████████| 128/128 [04:50<00:00,  2.27s/it, loss_G=3.942, loss_D=0.035]\n",
      "Epoch [5/10]: 100%|██████████| 128/128 [04:49<00:00,  2.26s/it, loss_G=4.073, loss_D=0.185]\n",
      "Epoch [6/10]: 100%|██████████| 128/128 [04:50<00:00,  2.27s/it, loss_G=3.488, loss_D=0.115]\n",
      "Epoch [7/10]: 100%|██████████| 128/128 [04:50<00:00,  2.27s/it, loss_G=3.468, loss_D=0.090]\n",
      "Epoch [8/10]: 100%|██████████| 128/128 [04:49<00:00,  2.26s/it, loss_G=4.102, loss_D=0.090]\n",
      "Epoch [9/10]: 100%|██████████| 128/128 [04:47<00:00,  2.25s/it, loss_G=5.112, loss_D=0.089]\n",
      "Epoch [10/10]: 100%|██████████| 128/128 [04:46<00:00,  2.24s/it, loss_G=2.768, loss_D=0.519]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# =========================================================\n",
    "# 6️⃣ Training Loop\n",
    "# =========================================================\n",
    "n_epochs = 10\n",
    "lambda_cycle = 10.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "    for real_X, real_Y in loop:\n",
    "        real_X, real_Y = real_X.to(device), real_Y.to(device)\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Generators\n",
    "        # -----------------------\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        fake_Y = G_XtoY(real_X)\n",
    "        fake_X = G_YtoX(real_Y)\n",
    "\n",
    "        # Adversarial losses\n",
    "        loss_G_adv_Y = adv_criterion(D_Y(fake_Y), torch.ones_like(D_Y(fake_Y)))\n",
    "        loss_G_adv_X = adv_criterion(D_X(fake_X), torch.ones_like(D_X(fake_X)))\n",
    "        loss_G_adv = (loss_G_adv_X + loss_G_adv_Y)\n",
    "\n",
    "        # Cycle-consistency\n",
    "        recov_X = G_YtoX(fake_Y)\n",
    "        recov_Y = G_XtoY(fake_X)\n",
    "        loss_cycle = cycle_criterion(recov_X, real_X) + cycle_criterion(recov_Y, real_Y)\n",
    "\n",
    "        # Total Generator Loss\n",
    "        loss_G = loss_G_adv + lambda_cycle * loss_cycle\n",
    "        loss_G.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminators\n",
    "        # -----------------------\n",
    "        opt_D.zero_grad()\n",
    "\n",
    "        # D_X\n",
    "        loss_D_X_real = adv_criterion(D_X(real_X), torch.ones_like(D_X(real_X)))\n",
    "        loss_D_X_fake = adv_criterion(D_X(fake_X.detach()), torch.zeros_like(D_X(fake_X)))\n",
    "        loss_D_X = (loss_D_X_real + loss_D_X_fake) * 0.5\n",
    "\n",
    "        # D_Y\n",
    "        loss_D_Y_real = adv_criterion(D_Y(real_Y), torch.ones_like(D_Y(real_Y)))\n",
    "        loss_D_Y_fake = adv_criterion(D_Y(fake_Y.detach()), torch.zeros_like(D_Y(fake_Y)))\n",
    "        loss_D_Y = (loss_D_Y_real + loss_D_Y_fake) * 0.5\n",
    "\n",
    "        # Total Discriminator Loss\n",
    "        loss_D = loss_D_X + loss_D_Y\n",
    "        loss_D.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "        loop.set_postfix({\n",
    "            \"loss_G\": f\"{loss_G.item():.3f}\",\n",
    "            \"loss_D\": f\"{loss_D.item():.3f}\"\n",
    "        })\n",
    "\n",
    "        # ==============================\n",
    "        # Logging\n",
    "        # ==============================\n",
    "\n",
    "    # Save example outputs\n",
    "    save_image(real_X, f\"out/real_X_epoch{epoch}.png\")\n",
    "    save_image(fake_Y, f\"out/fake_Y_epoch{epoch}.png\")\n",
    "    save_image(recov_X, f\"out/rec_X_epoch{epoch}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0cf980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G_XtoY.state_dict(), \"model/G_XtoY.pth\")\n",
    "torch.save(G_YtoX.state_dict(), \"model/G_YtoX.pth\")\n",
    "torch.save(D_X.state_dict(), \"model/D_X.pth\")\n",
    "torch.save(D_Y.state_dict(), \"model/D_Y.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildlife-da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
