{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a48e65",
   "metadata": {},
   "source": [
    "Download Missouri Camera Trap Image\n",
    "\n",
    "`aws s3 cp --recursive --no-sign-request \"s3://us-west-2.opendata.source.coop/agentmorris/lila-wildlife/missouricameratraps/images\" \"./images\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "183ce259",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_PATH = \"missouri_camera_traps_set1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5caf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def check_greyscale(img, threshold=1):\n",
    "    arr = np.asarray(img, dtype=np.float32)\n",
    "    diff_rg = np.abs(arr[...,0] - arr[...,1])\n",
    "    diff_rb = np.abs(arr[...,0] - arr[...,2])\n",
    "    diff_gb = np.abs(arr[...,1] - arr[...,2])\n",
    "    mean_diff = (diff_rg.mean() + diff_rb.mean() + diff_gb.mean()) / 3.0\n",
    "    return mean_diff < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "31993f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(JSON_PATH, 'r') as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "ann_img = ann[\"images\"]\n",
    "train_idx = []\n",
    "test_idx = []\n",
    "for i, img_info in enumerate(ann_img):\n",
    "    if \"n_boxes\" in img_info.keys():\n",
    "        test_idx.append(i)\n",
    "    else:\n",
    "        train_idx.append(i)\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(train_idx)\n",
    "np.random.shuffle(test_idx)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_idx, test_idx = train_test_split(range(len(ann[\"images\"])), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2d1420e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_dataset = \u001b[43mCCTDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mJSON_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mCCTDataset.__init__\u001b[39m\u001b[34m(self, image_dir, json_path, num_samples, transform, image_size, mode)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(full_path):\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# skip missing files\u001b[39;00m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcheck_greyscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.len_grey < num_samples:\n\u001b[32m     58\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mcheck_greyscale\u001b[39m\u001b[34m(img, threshold)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_greyscale\u001b[39m(img, threshold=\u001b[32m1\u001b[39m):\n\u001b[32m      5\u001b[39m     arr = np.asarray(img, dtype=np.float32)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     diff_rg = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     diff_rb = np.abs(arr[...,\u001b[32m0\u001b[39m] - arr[...,\u001b[32m2\u001b[39m])\n\u001b[32m      8\u001b[39m     diff_gb = np.abs(arr[...,\u001b[32m1\u001b[39m] - arr[...,\u001b[32m2\u001b[39m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_dataset = CCTDataset(\n",
    "    image_dir= \"images\",\n",
    "    json_path= JSON_PATH,\n",
    "    num_samples=1024,\n",
    "    image_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e2e3bc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greyscale img count: 32, Colour img count: 32, using dataset length 32\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CCTDataset(\n",
    "    image_dir= \"images\",\n",
    "    json_path= JSON_PATH,\n",
    "    num_samples=32,\n",
    "    image_size=512,\n",
    "    mode=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "99bfe38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbba44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ U-Net Generator\n",
    "# =========================================================\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_dropout=False):\n",
    "        super().__init__()\n",
    "        if down:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        self.use_dropout = use_dropout\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.down1 = UNetBlock(in_channels, 64, down=True)\n",
    "        self.down2 = UNetBlock(64, 128, down=True)\n",
    "        self.down3 = UNetBlock(128, 256, down=True)\n",
    "        self.down4 = UNetBlock(256, 512, down=True, use_dropout=True)\n",
    "        self.down5 = UNetBlock(512, 512, down=True, use_dropout=True)\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up1 = UNetBlock(512, 512, down=False, use_dropout=True)\n",
    "        self.up2 = UNetBlock(1024, 512, down=False, use_dropout=True)\n",
    "        self.up3 = UNetBlock(1024, 256, down=False)\n",
    "        self.up4 = UNetBlock(512, 128, down=False)\n",
    "        self.up5 = UNetBlock(256, 64, down=False)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, out_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        bottleneck = self.bottleneck(d5)\n",
    "        u1 = self.up1(bottleneck)\n",
    "        u2 = self.up2(torch.cat([u1, d5], 1))\n",
    "        u3 = self.up3(torch.cat([u2, d4], 1))\n",
    "        u4 = self.up4(torch.cat([u3, d3], 1))\n",
    "        u5 = self.up5(torch.cat([u4, d2], 1))\n",
    "        out = self.final(torch.cat([u5, d1], 1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669d1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# 2️⃣ PatchGAN Discriminator\n",
    "# =========================================================\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9b5d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 3️⃣ Setup\n",
    "# ================================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "G_XtoY = UNetGenerator().to(device)\n",
    "G_YtoX = UNetGenerator().to(device)\n",
    "D_X = PatchDiscriminator().to(device)\n",
    "D_Y = PatchDiscriminator().to(device)\n",
    "\n",
    "# Losses\n",
    "adv_criterion = nn.MSELoss()\n",
    "cycle_criterion = nn.L1Loss()\n",
    "\n",
    "# Optimizers\n",
    "opt_G = optim.Adam(\n",
    "    list(G_XtoY.parameters()) + list(G_YtoX.parameters()),\n",
    "    lr=2e-4, betas=(0.5, 0.999)\n",
    ")\n",
    "opt_D = optim.Adam(\n",
    "    list(D_X.parameters()) + list(D_Y.parameters()),\n",
    "    lr=2e-4, betas=(0.5, 0.999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa8fa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 256/256 [09:43<00:00,  2.28s/it, loss_G=3.006, loss_D=0.182]\n",
      "Epoch [2/10]: 100%|██████████| 256/256 [09:44<00:00,  2.28s/it, loss_G=4.304, loss_D=0.107]\n",
      "Epoch [3/10]: 100%|██████████| 256/256 [09:46<00:00,  2.29s/it, loss_G=3.059, loss_D=0.045] \n",
      "Epoch [4/10]: 100%|██████████| 256/256 [09:39<00:00,  2.27s/it, loss_G=4.606, loss_D=0.158]\n",
      "Epoch [5/10]: 100%|██████████| 256/256 [09:39<00:00,  2.26s/it, loss_G=4.363, loss_D=0.232]\n",
      "Epoch [6/10]: 100%|██████████| 256/256 [09:41<00:00,  2.27s/it, loss_G=3.606, loss_D=0.141]\n",
      "Epoch [7/10]: 100%|██████████| 256/256 [09:38<00:00,  2.26s/it, loss_G=4.132, loss_D=0.029]\n",
      "Epoch [8/10]: 100%|██████████| 256/256 [09:41<00:00,  2.27s/it, loss_G=4.054, loss_D=0.045]\n",
      "Epoch [9/10]: 100%|██████████| 256/256 [09:36<00:00,  2.25s/it, loss_G=3.849, loss_D=0.011]\n",
      "Epoch [10/10]: 100%|██████████| 256/256 [09:36<00:00,  2.25s/it, loss_G=3.459, loss_D=0.051]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# =========================================================\n",
    "# 6️⃣ Training Loop\n",
    "# =========================================================\n",
    "n_epochs = 10\n",
    "lambda_cycle = 10.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "    for real_X, real_Y in loop:\n",
    "        real_X, real_Y = real_X.to(device), real_Y.to(device)\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Generators\n",
    "        # -----------------------\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        fake_Y = G_XtoY(real_X)\n",
    "        fake_X = G_YtoX(real_Y)\n",
    "\n",
    "        # Adversarial losses\n",
    "        loss_G_adv_Y = adv_criterion(D_Y(fake_Y), torch.ones_like(D_Y(fake_Y)))\n",
    "        loss_G_adv_X = adv_criterion(D_X(fake_X), torch.ones_like(D_X(fake_X)))\n",
    "        loss_G_adv = (loss_G_adv_X + loss_G_adv_Y)\n",
    "\n",
    "        # Cycle-consistency\n",
    "        recov_X = G_YtoX(fake_Y)\n",
    "        recov_Y = G_XtoY(fake_X)\n",
    "        loss_cycle = cycle_criterion(recov_X, real_X) + cycle_criterion(recov_Y, real_Y)\n",
    "\n",
    "        # Total Generator Loss\n",
    "        loss_G = loss_G_adv + lambda_cycle * loss_cycle\n",
    "        loss_G.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminators\n",
    "        # -----------------------\n",
    "        opt_D.zero_grad()\n",
    "\n",
    "        # D_X\n",
    "        loss_D_X_real = adv_criterion(D_X(real_X), torch.ones_like(D_X(real_X)))\n",
    "        loss_D_X_fake = adv_criterion(D_X(fake_X.detach()), torch.zeros_like(D_X(fake_X)))\n",
    "        loss_D_X = (loss_D_X_real + loss_D_X_fake) * 0.5\n",
    "\n",
    "        # D_Y\n",
    "        loss_D_Y_real = adv_criterion(D_Y(real_Y), torch.ones_like(D_Y(real_Y)))\n",
    "        loss_D_Y_fake = adv_criterion(D_Y(fake_Y.detach()), torch.zeros_like(D_Y(fake_Y)))\n",
    "        loss_D_Y = (loss_D_Y_real + loss_D_Y_fake) * 0.5\n",
    "\n",
    "        # Total Discriminator Loss\n",
    "        loss_D = loss_D_X + loss_D_Y\n",
    "        loss_D.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "        loop.set_postfix({\n",
    "            \"loss_G\": f\"{loss_G.item():.3f}\",\n",
    "            \"loss_D\": f\"{loss_D.item():.3f}\"\n",
    "        })\n",
    "\n",
    "        # ==============================\n",
    "        # Logging\n",
    "        # ==============================\n",
    "\n",
    "    # Save example outputs\n",
    "    save_image(real_X, f\"out/real_X_epoch{epoch}.png\")\n",
    "    save_image(fake_Y, f\"out/fake_Y_epoch{epoch}.png\")\n",
    "    save_image(recov_X, f\"out/rec_X_epoch{epoch}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0cf980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G_XtoY.state_dict(), \"model/G_XtoY.pth\")\n",
    "torch.save(G_YtoX.state_dict(), \"model/G_YtoX.pth\")\n",
    "torch.save(D_X.state_dict(), \"model/D_X.pth\")\n",
    "torch.save(D_Y.state_dict(), \"model/D_Y.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9c2015f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_XtoY.load_state_dict(torch.load(\"model/G_XtoY.pth\"))\n",
    "G_YtoX.load_state_dict(torch.load(\"model/G_YtoX.pth\"))\n",
    "D_X.load_state_dict(torch.load(\"model/D_X.pth\"))\n",
    "D_Y.load_state_dict(torch.load(\"model/D_Y.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0909994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model v5a.0.1 already exists and is valid at C:\\Users\\aidan\\AppData\\Local\\Temp\\megadetector_models\\md_v5a.0.1.pt\n",
      "PyTorch reports 1 available CUDA devices\n",
      "GPU available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aidan\\anaconda3\\envs\\wildlife-da\\Lib\\site-packages\\yolov5\\utils\\general.py:31: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources as pkg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PT detector with compatibility mode classic\n",
      "Loaded image size 1280 from model metadata\n",
      "Using model stride: 64\n",
      "PTDetector using device cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Fusing layers... \n",
      "Model summary: 733 layers, 140054656 parameters, 0 gradients, 208.8 GFLOPs\n",
      "Model summary: 733 layers, 140054656 parameters, 0 gradients, 208.8 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from megadetector.detection import run_detector\n",
    "from megadetector.visualization import visualization_utils as vis_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "detector = run_detector.load_detector('MDV5A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4a7c08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_map = {\"1\": \"Animal\", \"2\": \"Person\", \"3\": 'Vehicle'}\n",
    "\n",
    "def draw_result(img, result, path, display=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    for det in result:\n",
    "        x1, y1, w, h = np.array(det[\"bbox\"]) * 512\n",
    "        acc = det[\"conf\"]\n",
    "        cat = cat_map[det[\"category\"]]\n",
    "        rect = patches.Rectangle((x1, y1), w, h, linewidth=1, edgecolor='red', facecolor='none')\n",
    "        ax.text(x1+7, y1-10, f\"{cat}: {acc*100:.1f}%\", color=\"black\", fontsize=8, backgroundcolor=\"red\")\n",
    "        ax.add_patch(rect)\n",
    "    ax.axis(\"off\")\n",
    "    plt.savefig(path, bbox_inches=\"tight\", pad_inches=0)\n",
    "    if display:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "75949a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 1 detections above threshold for grey image\n",
      "Found 1 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 0\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 0\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 0\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 1 detections above threshold for grey image\n",
      "Found 1 detections above threshold for generated image\n",
      "Bounding box count: 0\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 1 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 1 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 1 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 1 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 0\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 0\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 1 detections above threshold for grey image\n",
      "Found 1 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 1 detections above threshold for generated image\n",
      "Bounding box count: 0\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 1 detections above threshold for grey image\n",
      "Found 1 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 1 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 1 detections above threshold for generated image\n",
      "Bounding box count: 2\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 0\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 0\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 0 detections above threshold for generated image\n",
      "Bounding box count: 1\n",
      "Found 0 detections above threshold for grey image\n",
      "Found 1 detections above threshold for generated image\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"eval\", exist_ok=True)\n",
    "guess_X = [0, 0, 0, 0] # TN FN FP TP\n",
    "guess_Y = [0, 0, 0, 0]\n",
    "threshold = 0.25\n",
    "for i, (real_X, real_Y, cat_X) in enumerate(test_loader):\n",
    "    save_image(real_X, f\"eval/{i:02d}_real_X.png\")\n",
    "    fake_Y = G_XtoY(real_X.to(device))\n",
    "    save_image(fake_Y, f\"eval/{i:02d}_fake_Y.png\")\n",
    "    \n",
    "    img_X = vis_utils.load_image(f\"eval/{i:02d}_real_X.png\")\n",
    "    img_Y = vis_utils.load_image(f\"eval/{i:02d}_fake_Y.png\")\n",
    "\n",
    "    result_X = detector.generate_detections_one_image(img_X)\n",
    "    result_Y = detector.generate_detections_one_image(img_Y)\n",
    "    count = cat_X.cpu().numpy()[0]\n",
    "    print(f\"Bounding box count: {count}\")\n",
    "    det_X = [d for d in result_X['detections'] if d['conf'] > threshold]\n",
    "    print('Found {} detections above threshold for grey image'.format(len(det_X)))\n",
    "    # print(det_X)\n",
    "    draw_result(img_X, det_X, f\"eval/{i:02d}_real_X.png\", )\n",
    "\n",
    "    det_Y = [d for d in result_Y['detections'] if d['conf'] > threshold]\n",
    "    print('Found {} detections above threshold for generated image'.format(len(det_Y)))\n",
    "    # print(det_Y)\n",
    "    draw_result(img_Y, det_Y, f\"eval/{i:02d}_fake_Y.png\")\n",
    "\n",
    "    guess_X[(len(det_X) > 0)*2+(count > 0)] += 1\n",
    "    guess_Y[(len(det_Y) > 0)*2+(count > 0)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6ef41afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on real grey images: 0.438\n",
      "Precision on real grey images: 1.000\n",
      "Recall on real grey images: 0.217\n",
      "Accuracy on generated images: 0.625\n",
      "Precision on generated images: 1.000\n",
      "Recall on generated images: 0.478\n"
     ]
    }
   ],
   "source": [
    "total = len(test_loader)\n",
    "print(f\"Accuracy on real grey images: {(guess_X[0]+guess_X[3])/total:.3f}\")\n",
    "print(f\"Precision on real grey images: {guess_X[3]/(guess_X[2]+guess_X[3]):.3f}\")\n",
    "print(f\"Recall on real grey images: {guess_X[3]/(guess_X[1]+guess_X[3]):.3f}\")\n",
    "print(f\"Accuracy on generated images: {(guess_Y[0]+guess_Y[3])/total:.3f}\")\n",
    "print(f\"Precision on generated images: {guess_Y[3]/(guess_Y[2]+guess_Y[3]):.3f}\")\n",
    "print(f\"Recall on generated images: {guess_Y[3]/(guess_Y[1]+guess_Y[3]):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildlife-da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
